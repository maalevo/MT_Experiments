{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import all necessary libraries ##\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "#########################\n",
    "\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
    "from pm4py.algo.filtering.log.variants import variants_filter\n",
    "from pm4py.statistics.traces.log import case_statistics\n",
    "from pm4py.objects.log.util import insert_classifier\n",
    "\n",
    "#########################\n",
    "\n",
    "import pomegranate as pom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log:\n",
    "    \n",
    "    ### SETUP ###\n",
    "    \n",
    "    ## load dataset, generate mapping and generate strings\n",
    "    def __init__(self, path):\n",
    "        self.strings = []\n",
    "        self.transl = {}\n",
    "        \n",
    "        self.log = xes_importer.apply(path)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            # generate custom activity classifier\n",
    "            self.log, activity_key = insert_classifier.insert_activity_classifier_attribute(self.log, \"Activity classifier\")\n",
    "            for trace in self.log:\n",
    "                for event in trace:\n",
    "                    custom_classifier = \"\"\n",
    "                    for activity_classifier in self.log.classifiers[\"Activity classifier\"]:\n",
    "                        custom_classifier = custom_classifier + event[activity_classifier] + \"+\"\n",
    "                    custom_classifier = custom_classifier[:-1]\n",
    "                    event[\"customClassifier\"] = custom_classifier\n",
    "        except:\n",
    "            for trace in self.log:\n",
    "                for event in trace:\n",
    "                    event[\"customClassifier\"] = event[\"concept:name\"]\n",
    "    \n",
    "        self.clear_caches()\n",
    "        self.gen_mapping()\n",
    "        self.read_lof()\n",
    "        \n",
    "    def clear_caches(self):\n",
    "        self.Nk_res_dict = {} # N_k result cache\n",
    "    \n",
    "    ## generate mapping from activity to char\n",
    "    def gen_mapping(self):\n",
    "        ## generate mapping from activities to chars ##\n",
    "        activities = list(attributes_filter.get_attribute_values(self.log, \"customClassifier\").keys())\n",
    "        \n",
    "        for i, a in enumerate(activities):\n",
    "            self.transl[a] = str(i+1)\n",
    "    \n",
    "    def read_lof(self):\n",
    "        self.lof = np.genfromtxt(path + \".csv\")\n",
    "        self.anomaly = []\n",
    "        \n",
    "        cutoff = np.percentile(self.lof, 90)\n",
    "        \n",
    "        for i, l in enumerate(self.lof):\n",
    "            if l >= cutoff:\n",
    "                self.anomaly.append([\"anomaly\"] * len(self.log[i]))\n",
    "            else:\n",
    "                self.anomaly.append([\"no anomaly\"] * len(self.log[i]))\n",
    "\n",
    "                \n",
    "    def gen_lists(self):\n",
    "        lists = []\n",
    "        for trace in self.log:\n",
    "            tlist = np.empty(len(trace))\n",
    "            for i, event in enumerate(trace):\n",
    "                tlist[i] = self.transl[event[\"customClassifier\"]]\n",
    "            lists.append(tlist)\n",
    "        return lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733d1eefba0640188a616107c10a5d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"Datasets/BPIC13.xes\"\n",
    "\n",
    "# import log\n",
    "tlog = log(path)\n",
    "\n",
    "lists = tlog.gen_lists()\n",
    "is_anomaly = tlog.anomaly\n",
    "\n",
    "# split data into training and test\n",
    "lists_train, lists_test, is_anomaly_train, is_anomaly_test = ms.train_test_split(lists, is_anomaly, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pom.HiddenMarkovModel.from_samples(\n",
    "    pom.DiscreteDistribution, \n",
    "    n_components=2,\n",
    "    X=lists_train, \n",
    "    labels=is_anomaly_train,\n",
    "    #state_names=[\"anomaly\", \"no anomaly\", \"unknown\"],\n",
    "    state_names=[\"anomaly\", \"no anomaly\"],\n",
    "    algorithm=\"labeled\")\n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive = 0 # anomaly incorrectly detected\n",
    "false_negative = 0 # anomaly incrrectly not detected\n",
    "true_positive = 0 # anomaly correctly detectd\n",
    "true_negative = 0 # anomaly correclty not detected\n",
    "\n",
    "for i, t in enumerate(lists_test):\n",
    "    prediction = model.predict(t)\n",
    "    if prediction[-1] == 0 and is_anomaly_test[i][-1] == \"no anomaly\":\n",
    "        true_negative = true_negative + 1\n",
    "    if prediction[-1] == 0 and is_anomaly_test[i][-1] == \"anomaly\":\n",
    "        false_negative = false_negative + 1\n",
    "    if prediction[-1] == 1 and is_anomaly_test[i][-1] == \"no anomaly\":\n",
    "        false_positive = false_positive + 1\n",
    "    if prediction[-1] == 1 and is_anomaly_test[i][-1] == \"anomaly\":\n",
    "        true_positive = true_positive + 1\n",
    "\n",
    "TPR = true_positive / (true_positive + false_negative)\n",
    "TNR = true_negative / (true_negative + false_positive)\n",
    "        \n",
    "FPR = false_positive / (false_positive + true_negative)\n",
    "FNR = false_negative / (false_negative + true_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR\t\t\t0.8%\n",
      "FNR\t\t\t90.6%\n",
      "\n",
      "\n",
      "TNR (sensitivity)\t99.2%\n",
      "TPR (specificity)\t9.4%\n"
     ]
    }
   ],
   "source": [
    "print(\"FPR\\t\\t\\t\" + str(round(FPR * 100, 1)) + \"%\")\n",
    "print(\"FNR\\t\\t\\t\" + str(round(FNR * 100, 1)) + \"%\")\n",
    "print(\"\\n\")\n",
    "print(\"TNR (sensitivity)\\t\" + str(round(TNR * 100, 1)) + \"%\")\n",
    "print(\"TPR (specificity)\\t\" + str(round(TPR * 100, 1)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
